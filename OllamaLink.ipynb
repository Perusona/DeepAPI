{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1.ollama库调用ollama\n",
    "- 2.langchain调用ollama\n",
    "- 3.requests调用ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.olllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=ollama.chat(model=\"deepseek-r1:1.5b\",stream=False,messages=[{\"role\": \"user\",\"content\": \"你是谁\"}],options={\"temperature\":0})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.model,\"\\n************************************\")\n",
    "print(res.message.role,\"\\n************************************\\n\")\n",
    "print(res.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "host=\"localhost\"\n",
    "port=\"11434\"\n",
    "llm=Ollama(base_url=f\"http://{host}:{port}\",model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "# 定义提示模板\n",
    "template = \"请回答以下问题：{question}\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# 创建 LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# 运行链\n",
    "question = \"什么是人工智能？\"\n",
    "response = chain.run(question)\n",
    "\n",
    "# 输出结果\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "您好！我是由中国的深度求索（DeepSeek）公司开发的智能助手DeepSeek-R1。如您有任何任何问题，我会尽我所能为您提供帮助。\n",
      "完整回复: <think>\n",
      "\n",
      "</think>\n",
      "\n",
      "您好！我是由中国的深度求索（DeepSeek）公司开发的智能助手DeepSeek-R1。如您有任何任何问题，我会尽我所能为您提供帮助。\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "host=\"localhost\"\n",
    "port=\"11434\"\n",
    "url=f\"http://{host}:{port}/api/chat\"\n",
    "model=\"deepseek-r1:1.5b\"\n",
    "headers = { 'Content-Type': 'application/json' }\n",
    "data={\n",
    "    \"model\":model,\n",
    "    \"options\":{\n",
    "        \"temperature\":0\n",
    "        },\n",
    "    \"strem\":True,\n",
    "    \"messages\":[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"你是谁\"\n",
    "        }],\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post(url, json=data, headers=headers, timeout=60, stream=True)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    full_response = []\n",
    "    # 逐行解析流式响应\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            try:\n",
    "                chunk = json.loads(line.decode('utf-8'))\n",
    "                if chunk.get(\"done\", False):\n",
    "                    # 最终合并所有内容\n",
    "                    final_content = \"\".join(full_response)\n",
    "                    print(\"\\n完整回复:\", final_content)\n",
    "                elif \"message\" in chunk and \"content\" in chunk[\"message\"]:\n",
    "                    content = chunk[\"message\"][\"content\"]\n",
    "                    full_response.append(content)\n",
    "                    # 实时输出流式内容\n",
    "                    print(content, end=\"\", flush=True)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"\\nJSON解析错误: {e}\")\n",
    "                print(\"原始数据块:\", line)\n",
    "\n",
    "except requests.exceptions.HTTPError as http_err:\n",
    "    print(f\"\\nHTTP错误: {http_err}\")\n",
    "    if response:\n",
    "        print(f\"状态码: {response.status_code}\")\n",
    "        print(\"错误响应内容:\", response.text)\n",
    "except requests.exceptions.RequestException as req_err:\n",
    "    print(f\"\\n请求异常: {req_err}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n未知错误: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
